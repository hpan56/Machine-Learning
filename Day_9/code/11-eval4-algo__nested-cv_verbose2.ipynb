{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479: Machine Learning (Fall 2019)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-fs2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L11: Model Evaluation 4 -- Algorithm Comparison (Nested Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## -- verbose version 2 (using `cross_validate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to implement nested cross-validation in scikit-learn. This notebook is a more compact version of the other notebook [./11-eval4-algo__nested-cv_verbose1.ipynb](./11-eval4-algo__nested-cv_verbose1.ipynb). Here, instead of using `StratifiedKFold` directly and iterate over the splits, we use the `cross_validate` function.\n",
    "\n",
    "![./nested-cv-image.png](nested-cv-image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka 2019-11-07 \n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.9.0\n",
      "\n",
      "sklearn 0.21.3\n",
      "mlxtend 0.17.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -d -p sklearn,mlxtend -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.data import mnist_data\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Loading and splitting the dataset\n",
    "# Note that this is a small (stratified) subset\n",
    "# of MNIST; it consists of 5000 samples only, that is,\n",
    "# 10% of the original MNIST dataset\n",
    "# http://yann.lecun.com/exdb/mnist/\n",
    "X, y = mnist_data()\n",
    "X = X.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=y)\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(multi_class='multinomial',\n",
    "                          solver='newton-cg',\n",
    "                          random_state=1)\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "clf3 = DecisionTreeClassifier(random_state=1)\n",
    "clf4 = SVC(random_state=1)\n",
    "clf5 = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Building the pipelines\n",
    "pipe1 = Pipeline([('std', StandardScaler()),\n",
    "                  ('clf1', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('std', StandardScaler()),\n",
    "                  ('clf2', clf2)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),\n",
    "                  ('clf4', clf4)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'clf1__penalty': ['l2'],\n",
    "                'clf1__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid2 = [{'clf2__n_neighbors': list(range(1, 10)),\n",
    "                'clf2__p': [1, 2]}]\n",
    "\n",
    "param_grid3 = [{'max_depth': list(range(1, 10)) + [None],\n",
    "                'criterion': ['gini', 'entropy']}]\n",
    "\n",
    "param_grid4 = [{'clf4__kernel': ['rbf'],\n",
    "                'clf4__C': np.power(10., np.arange(-4, 4)),\n",
    "                'clf4__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'clf4__kernel': ['linear'],\n",
    "                'clf4__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid5 = [{'n_estimators': [10, 100, 500, 1000, 10000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "gridcvs = {}\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2,\n",
    "                             param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, pipe2, clf3, pipe4, clf5),\n",
    "                            ('Softmax', 'KNN', 'DTree', 'SVM', 'RForest')):\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=-1,\n",
    "                       cv=inner_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: DTree\n",
      "    Inner loop:\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 72.28%\n",
      "        Best parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "        ACC (on outer test fold) 81.25%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 75.34%\n",
      "        Best parameters: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=9,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "        ACC (on outer test fold) 76.12%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 72.72%\n",
      "        Best parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=7,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "        ACC (on outer test fold) 78.88%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 74.16%\n",
      "        Best parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=7,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "        ACC (on outer test fold) 73.12%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 74.25%\n",
      "        Best parameters: DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=1, splitter='best')\n",
      "        ACC (on outer test fold) 77.25%\n",
      "\n",
      "DTree | outer ACC 77.33% +/- 2.72\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: KNN\n",
      "    Inner loop:\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.97%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf2',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=50,\n",
      "                                      metric='minkowski', metric_params=None,\n",
      "                                      n_jobs=None, n_neighbors=1, p=1,\n",
      "                                      weights='uniform'))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.62%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.69%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf2',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=50,\n",
      "                                      metric='minkowski', metric_params=None,\n",
      "                                      n_jobs=None, n_neighbors=1, p=1,\n",
      "                                      weights='uniform'))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.62%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.78%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf2',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=50,\n",
      "                                      metric='minkowski', metric_params=None,\n",
      "                                      n_jobs=None, n_neighbors=1, p=1,\n",
      "                                      weights='uniform'))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 93.00%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.75%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf2',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=50,\n",
      "                                      metric='minkowski', metric_params=None,\n",
      "                                      n_jobs=None, n_neighbors=4, p=1,\n",
      "                                      weights='uniform'))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.38%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 89.00%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf2',\n",
      "                 KNeighborsClassifier(algorithm='ball_tree', leaf_size=50,\n",
      "                                      metric='minkowski', metric_params=None,\n",
      "                                      n_jobs=None, n_neighbors=1, p=1,\n",
      "                                      weights='uniform'))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.88%\n",
      "\n",
      "KNN | outer ACC 91.10% +/- 0.96\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: RForest\n",
      "    Inner loop:\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 92.62%\n",
      "        Best parameters: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "        ACC (on outer test fold) 94.12%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 92.69%\n",
      "        Best parameters: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "        ACC (on outer test fold) 93.50%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 92.91%\n",
      "        Best parameters: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "        ACC (on outer test fold) 94.62%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 93.31%\n",
      "        Best parameters: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "        ACC (on outer test fold) 92.75%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 92.91%\n",
      "        Best parameters: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
      "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
      "                       warm_start=False)\n",
      "        ACC (on outer test fold) 94.50%\n",
      "\n",
      "RForest | outer ACC 93.90% +/- 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/miniconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: SVM\n",
      "    Inner loop:\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 89.97%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf4',\n",
      "                 SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3, gamma=0.001,\n",
      "                     kernel='rbf', max_iter=-1, probability=False,\n",
      "                     random_state=1, shrinking=True, tol=0.001,\n",
      "                     verbose=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 92.50%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 90.09%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf4',\n",
      "                 SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3, gamma=0.001,\n",
      "                     kernel='rbf', max_iter=-1, probability=False,\n",
      "                     random_state=1, shrinking=True, tol=0.001,\n",
      "                     verbose=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 91.88%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 90.53%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf4',\n",
      "                 SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3, gamma=0.001,\n",
      "                     kernel='rbf', max_iter=-1, probability=False,\n",
      "                     random_state=1, shrinking=True, tol=0.001,\n",
      "                     verbose=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 93.00%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 92.03%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf4',\n",
      "                 SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3,\n",
      "                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n",
      "                     probability=False, random_state=1, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.00%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 89.53%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf4',\n",
      "                 SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "                     decision_function_shape='ovr', degree=3,\n",
      "                     gamma='auto_deprecated', kernel='linear', max_iter=-1,\n",
      "                     probability=False, random_state=1, shrinking=True,\n",
      "                     tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 92.38%\n",
      "\n",
      "SVM | outer ACC 91.95% +/- 1.04\n",
      "-------------------------------------------------- \n",
      "\n",
      "Algorithm: Softmax\n",
      "    Inner loop:\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.00%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf1',\n",
      "                 LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='multinomial', n_jobs=None,\n",
      "                                    penalty='l2', random_state=1,\n",
      "                                    solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 91.88%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.25%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf1',\n",
      "                 LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='multinomial', n_jobs=None,\n",
      "                                    penalty='l2', random_state=1,\n",
      "                                    solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.62%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 89.09%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf1',\n",
      "                 LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='multinomial', n_jobs=None,\n",
      "                                    penalty='l2', random_state=1,\n",
      "                                    solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.38%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 89.72%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf1',\n",
      "                 LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='multinomial', n_jobs=None,\n",
      "                                    penalty='l2', random_state=1,\n",
      "                                    solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 88.12%\n",
      "\n",
      "        Best ACC (avg. of inner test folds) 88.22%\n",
      "        Best parameters: Pipeline(memory=None,\n",
      "         steps=[('std',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('clf1',\n",
      "                 LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='multinomial', n_jobs=None,\n",
      "                                    penalty='l2', random_state=1,\n",
      "                                    solver='newton-cg', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "        ACC (on outer test fold) 90.62%\n",
      "\n",
      "Softmax | outer ACC 90.32% +/- 1.22\n"
     ]
    }
   ],
   "source": [
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "\n",
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    scores_dict = cross_validate(gs_est, \n",
    "                                 X=X_train, \n",
    "                                 y=y_train, \n",
    "                                 cv=outer_cv,\n",
    "                                 return_estimator=True,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "    print(50 * '-', '\\n')\n",
    "    print('Algorithm:', name)\n",
    "    print('    Inner loop:')\n",
    "    \n",
    "    \n",
    "    for i in range(scores_dict['test_score'].shape[0]):\n",
    "\n",
    "        print('\\n        Best ACC (avg. of inner test folds) %.2f%%' % (scores_dict['estimator'][i].best_score_ * 100))\n",
    "        print('        Best parameters:', scores_dict['estimator'][i].best_estimator_)\n",
    "        print('        ACC (on outer test fold) %.2f%%' % (scores_dict['test_score'][i]*100))\n",
    "\n",
    "    print('\\n%s | outer ACC %.2f%% +/- %.2f' % \n",
    "          (name, scores_dict['test_score'].mean() * 100, \n",
    "           scores_dict['test_score'].std() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine the best algorithm from the experiment above; e.g., we find that Random Forest is performing best\n",
    "- Now, select a hyperparameters for the model based on regular k-fold on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=2, random_state=1, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False, random_state=1,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'n_estimators': [10, 100, 500, 1000, 10000]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv_model_select = GridSearchCV(estimator=clf5,\n",
    "                                param_grid=param_grid5,\n",
    "                                scoring='accuracy',\n",
    "                                n_jobs=-1,\n",
    "                                cv=inner_cv,\n",
    "                                verbose=1,\n",
    "                                refit=True)\n",
    "\n",
    "gcv_model_select.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 93.42% (average over k-fold CV test folds)\n",
      "Best Parameters: {'n_estimators': 1000}\n",
      "Training Accuracy: 100.00%\n",
      "Test Accuracy: 93.90%\n"
     ]
    }
   ],
   "source": [
    "best_model = gcv_model_select.best_estimator_\n",
    "\n",
    "\n",
    "## We can skip the next step because we set refit=True\n",
    "## so scikit-learn has already fit the model to the\n",
    "## whole training set\n",
    "\n",
    "# best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=best_model.predict(X_train))\n",
    "test_acc = accuracy_score(y_true=y_test, y_pred=best_model.predict(X_test))\n",
    "\n",
    "print('Accuracy %.2f%% (average over k-fold CV test folds)' %\n",
    "      (100 * gcv_model_select.best_score_))\n",
    "print('Best Parameters: %s' % gcv_model_select.best_params_)\n",
    "\n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "print('Test Accuracy: %.2f%%' % (100 * test_acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
