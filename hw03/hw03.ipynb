{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 (66 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479: Machine Learning (Fall 2019)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-fs2019/\n",
    "\n",
    "---\n",
    "\n",
    "**Due**: Dec 07, (before 11:59 pm).\n",
    "\n",
    "**How to submit**\n",
    "\n",
    "As mentioned in the lecture, you need to send the `.ipynb` file with your answers plus an `.html` file, which will serve as a backup for us in case the `.ipynb` file cannot be opened on my or the TA's computer. In addition, you may also export the notebook as PDF and upload it as well.\n",
    "\n",
    "The homework solution should be uploaded on Canvas. You can submit it as often as you like before the deadline.\n",
    "\n",
    "Note that there are 11 tasks, and the HW is worth 66 pts in total (`11*6pts = 66pts`).\n",
    "\n",
    "**Important**\n",
    "\n",
    "- The cells that require your code answer are marked with `\"# YOUR CODE\"` comments.\n",
    "- Note that you may use 1 or more line of code for replacing each `\"# YOUR CODE\"` comment.\n",
    "\n",
    "For example, imagine there is a question asking you to implement a threshold function that should return 1 if the input `x` is greater than 0.5 and otherwise. This could appear as follows in the the exercise:\n",
    "\n",
    "```python\n",
    "def threshold_func(x):\n",
    "    # YOUR CODE\n",
    "```\n",
    "\n",
    "A valid answer could be\n",
    "\n",
    "```python\n",
    "def threshold_func(x):\n",
    "    if x > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "```\n",
    "\n",
    "Another valid solution could be\n",
    "\n",
    "```python\n",
    "def threshold_func(x):\n",
    "    return int(x > 0.5)\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a 'Sebastian Raschka' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"paragraph\">\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "  <p><br></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Tuning and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Using Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will be working with the *Pima Indians Diabetes Database* database by  [Vincent Sigillito](vgs@aplcen.apl.jhu.edu), which is available from the UCI database (https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes) or OpenML (https://www.openml.org/d/37).\n",
    "\n",
    "\n",
    "The dataset contains information about 768 patients along with the Diabetes diagnosis. The Diabetes diagnosis is a binary label, where \"tested_positive\" means that a patient has diabetes and \"tested_negative\" means that a patient does not have diabetes.\n",
    "\n",
    "I additional to the class label, there are 8 numeric features in the dataset, which are listed below:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Load the Dataset [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas to load the dataset from the `dataset_37_diabetes.csv` CSV file located in the HW folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(# YOUR CODE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Preprocess the class label [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the class label using pandas `apply` or `map` method. The mapping should be as follows:\n",
    "\n",
    "- `'tested_positive'` should be converted to `1`\n",
    "- `'tested_negative'` should be converted to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "df['class'] = df['class'].#YOUR CODE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3) Split dataset into training and test sets [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset into 70% training and 30% test data\n",
    "- Perform a stratified split\n",
    "- use `0` as the random seed for shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "y = df['class'].values\n",
    "X = df.iloc[:, :-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4) Gridsearch and model selection [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to use `GridSearchCV` from scikit-learn to find the best parameters for `max_depth` and `criterion` for a decision tree. For max_depth, the values `[1, 2, 3, 4, 5, 10, 15, 20, None]` should be tried, and for criterion both Gini and Entropy should be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "param_grid = # YOUR CODE\n",
    "\n",
    "gs = GridSearchCV(estimator=tree,\n",
    "                  param_grid=param_grid,\n",
    "                  iid=False,\n",
    "                  n_jobs=-1,\n",
    "                  refit=True,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Accuracy: %.2f%%' % (gs.best_score_*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print the best hyperparameters obtained from the `GridSearchCV` run. Also, compute the accuracy the model, which uses the best hyperparameter settings and was trained on the whole training set, on the test set (`X_test`, `y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "print('Best Params: %s' % gs.best_params_)\n",
    "\n",
    "## model is already fit to the whole training set because  we used `refit=True` in GridSearchCV\n",
    "print('Test Accuracy: %.2f%%' % ( # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Estimate the Generalization Performance using Bootstrap Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are asked to compute the accuracy of the model from the previous exercise (1.1), using the train set (`X_train`, `y_train`), via different bootstrap methods. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Compare the Out-of-Bag, .632, and .632+ bootstrap approaches [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the bootstrap estimates and confidence intervals, you are going to use the `bootstrap_point632_score` function implemented in MLxtend: \n",
    "http://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/\n",
    "\n",
    "The accruacy should be the mean accuracy over the 200 bootstrap values that the `bootstrap_point632_score` method returns.\n",
    "\n",
    "- For this, use the best model you obtained from the previous exercise 1.1.4)\n",
    "- use 200 bootstrap rounds\n",
    "- set the random seed to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Compute Out-of-bag Bootstrap\n",
    "scores = bootstrap_point632_score(# YOUR CODE\n",
    "\n",
    "\n",
    "# Compute accuracy (average over the bootstrap rounds)\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# Compute the 95% confidence interval around the accuracy estimate\n",
    "lower = # YOUR CODE\n",
    "upper = # YOUR CODE\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Compute .632 Bootstrap\n",
    "scores = bootstrap_point632_score(# YOUR CODE\n",
    "\n",
    "\n",
    "# Compute accuracy (average over the bootstrap rounds)\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# Compute the 95% confidence interval around the accuracy estimate\n",
    "lower = # YOUR CODE\n",
    "upper = # YOUR CODE\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Compute .632+ Bootstrap\n",
    "scores = bootstrap_point632_score(# YOUR CODE\n",
    "\n",
    "\n",
    "# Compute accuracy (average over the bootstrap rounds)\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy: %.2f%%' % (100*acc))\n",
    "\n",
    "# Compute the 95% confidence interval around the accuracy estimate\n",
    "lower = # YOUR CODE\n",
    "upper = # YOUR CODE\n",
    "print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Analyzing the different bootstrap results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) [6 Pts] Based on what you have learned it class, which of the three bootstrap methods (out-of-bag, 0.632, or 0.632+) do you expect to yield a generalization accuracy estimate from the training set that is closest to the true generalization performance of the model? Explain your reasoning in 1-3 sentences. (Tip: Think about optimistic and pessimistic bias).\n",
    "\n",
    "\n",
    "< YOUR ANSWER >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) [6 Pts] Based on your observations from the experiment in 1.2.1), which bootstrap approach (out-of-bag, 0.632, or 0.632+) yields an accuracy estimate from the training dataset that is closest to the test set accuracy from exercise 1.1.4? Is this reasonable? Explain your answer in 1-3 sentences. Also, to answer this question, assume that the test set accuracy from 1.1.4) is a perfect estimate of the true generalization accuracy of the model. \n",
    "\n",
    "< YOUR ANSWER >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3) [6 Pts] Based on your observations from the experiment in 1.2.1), are the overall results consistent with what you expected in your answer above (question 1))? Explain your reasoning in 3-5 sentences. Also, to answer this question, assume that the test set accuracy from 1.1.4) is a perfect estimate of the true generalization accuracy of the model. \n",
    "\n",
    "Tip: Discuss which methods are optimistically and pessimistically biased and whether this was expected.\n",
    "\n",
    "< YOUR ANSWER >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Balanced Accuracy (Average Per-Class Accuracy) [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our discussion in class ([Lecture 12](https://github.com/rasbt/stat479-machine-learning-fs19/blob/master/12_eval5-metrics/12_eval5-metrics__slides.pdf) Slide 16 & 17), implement a function that computes the balanced accuracy (this is sometimes also called \"average per-class accuracy\"). You can implement the accuracy whatever way you like using Python and NumPy.  Below is a template that you can use and modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def balanced_accuracy(y_true, y_predicted):\n",
    "    \n",
    "    y_true_ary = np.array(y_true)\n",
    "    y_predicted_ary = np.array(y_predicted)\n",
    "    \n",
    "    # optional checks\n",
    "    assert len(y_true_ary.shape) == len(y_predicted_ary.shape)\n",
    "    assert y_true_ary.shape[0] == y_predicted_ary.shape[0]\n",
    "    \n",
    "    # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE BUT DO NOT MODIFY THIS CELL\n",
    "\n",
    "true_labels = np.array(3*[0] + 69*[1] + 18*[2])\n",
    "predicted_labels = np.array(10*[0] + 50*[1] + 30*[2])\n",
    "    \n",
    "balanced_accuracy(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Receiver Operater Characteristic (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will build your intuition for ROC by practicing how to implement it with scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plotting a ROC Curve [6 Pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are asked to plot a ROC curve. You are given a 2D array of probability values (`y_probabilities`; see next code cells) where \n",
    "\n",
    "- a value in the first column refers to the probability that a given test example (each row is one test example) belongs to class 0\n",
    "- a value in the second column refers to the probability that a given test example belongs to class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE BUT DO NOT MODIFY THIS CELL\n",
    "\n",
    "\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X, y = X[:100, [1]], y[:100]\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.5, shuffle=True, random_state=0, stratify=y)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=123)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "print(y_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, these scores are probabilities here, but scores can be obtained from an arbitrary classifier (ROC curves are not limited to logistic regression classifiers). For instance, in k-nearest neighbor classifiers, we can consider the fraction of the majority class labels and number of neighbors as the score. In decision tree classifiers, the score can be calculated as the ratio of the majority class labels and number of data points at a given node.\n",
    "\n",
    "(In case you are curious, 'lbfgs' stands for Limited-memory BFGS, which is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno; not important to know here though.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You should only use Python base functions, NumPy, and matplotlib to get full points (do not use other external libraries)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pos_label` argument is used to specify the positive label and the threshold. For instance, if we are given score\n",
    "0.8, this score refers to the \"probability\" of the positive label. Assuming that the positive label is 1, this refers to a 80% probability that the true class label is 1. \n",
    "\n",
    "- Note that in the `y_probabilities` array, the second column refers to the probabilities of class label 1.\n",
    "- The `plot_roc_curve` function should only receive a 1D array for `y_score`. E.g., \n",
    "\n",
    "if `y_probabilities` is \n",
    "\n",
    "```\n",
    "[[0.44001556 0.55998444]\n",
    " [0.69026364 0.30973636]\n",
    " [0.31814182 0.68185818]\n",
    " [0.56957726 0.43042274]\n",
    " [0.86339788 0.13660212]\n",
    " [0.56957726 0.43042274]\n",
    " [0.86339788 0.13660212]\n",
    " [0.44001556 0.55998444]\n",
    " [0.08899234 0.91100766]\n",
    " [0.50487831 0.49512169]\n",
    " [0.74306586 0.25693414]\n",
    "```\n",
    " \n",
    "The `y_score` array is expected to be \n",
    "\n",
    "a) `y_score = [0.5599..., 0.3097..., 0.6818..., 0.4304..., ...]` for `pos_label=1`\n",
    "\n",
    "and \n",
    "\n",
    "b) `y_score = [0.4400..., 0.6902..., 0.3181..., 0.5695..., ...]` for `pos_label=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_score, pos_label=1, num_thresholds=100):\n",
    "\n",
    "    y_true_ary = np.array(y_true)\n",
    "    y_score_ary = np.array(y_score)\n",
    "    x_axis_values = []\n",
    "    y_axis_values = []\n",
    "    thresholds = np.linspace(0., 1., num_thresholds)\n",
    "\n",
    "    num_positives = np.sum(y_true == pos_label)\n",
    "    num_negatives = y_true.shape[0] - num_positives\n",
    "\n",
    "    for i, thr in enumerate(thresholds):\n",
    "        \n",
    "        binarized_scores = # YOUR CODE\n",
    "        \n",
    "        positive_predictions = (binarized_scores == pos_label)\n",
    "        \n",
    "        num_true_positives = # YOUR CODE\n",
    "        num_false_positives = # YOUR CODE\n",
    "        \n",
    "        x_axis_values.append(num_false_positives / float(num_negatives))\n",
    "        y_axis_values.append(num_true_positives / float(num_positives))\n",
    "\n",
    "    plt.step(x_axis_values, y_axis_values, where='post')\n",
    "    \n",
    "    plt.xlim([0., 1.01])\n",
    "    plt.ylim([0., 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXECUTE BUT DO NOT MODIFY THIS CELL\n",
    "\n",
    "plot_roc_curve(y_test, y_probabilities[:, 1], pos_label=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE BUT DO NOT MODIFY THIS CELL\n",
    "\n",
    "plot_roc_curve(y_test, y_probabilities[:, 0], pos_label=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [6 Pts] Calculating the ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are asked to modify your previous `plot_roc_curve` function to compute the ROC area under the curve (ROC AUC). To compute the ROC AUC, you can use NumPy's `trapz` function for your convenience (https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.trapz.html).\n",
    "\n",
    "- As before, you should only use basic Python functions, NumPy, and matplotlib to get full points for this exercise (do not use other external libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------#\n",
    "##### STUDENTS #####\n",
    "#------------------#\n",
    "\n",
    "def plot_roc_curve_plus_auc(y_true, y_score, pos_label=1, num_thresholds=100):\n",
    "\n",
    "    # INSERT YOUR CODE FROM THE PREVIOUS EXERCISE HERE\n",
    "    # BUT MODIFY IT SUCH THAT IT ALSO RETURNS THE\n",
    "    # ROC Area Under the Curve\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Calculate the ROC AUC for the positive class label 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T MODIFY BUT EXECUTE THIS CELL TO SHOW YOUR SOLUTION\n",
    "\n",
    "auc = plot_roc_curve_plus_auc(y_test, y_probabilities[:, 0], pos_label=0)\n",
    "print('ROC AUC: %.4f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculate the ROC AUC for the positive class label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T MODIFY BUT EXECUTE THIS CELL TO SHOW YOUR SOLUTION\n",
    "\n",
    "auc = plot_roc_curve_plus_auc(y_test, y_probabilities[:, 1], pos_label=1)\n",
    "print('ROC AUC: %.4f' % auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
